{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4LxkTHzuo3RhRP/ancc0t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hind190/Data-Mining-Project-/blob/main/Phase2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2: Data Preprocessing**\n",
        "\n",
        "In this part, we apply a series of data preprocessing techniques to prepare the dataset for accurate and reliable analysis. The techniques used include Discretization, Noise Removal, Handling Missing Values, and Normalization. These techniques were chosen based on the structure of the dataset and the analytical requirements.\n",
        "\n",
        "For each technique, we provide an explanation of why it was necessary, how it was implemented, and which attributes it was applied to. We also include a brief description of the results, outlining how the dataset improved as a result of these transformations. These preprocessing steps help reduce inconsistencies, minimize the impact of noise, balance feature scales, and improve interpretability for downstream tasks such as K-Means clustering and Decision Tree classification.\n",
        "\n",
        "Finally, snapshots of both the raw dataset and the preprocessed dataset are provided to clearly demonstrate the changes made."
      ],
      "metadata": {
        "id": "a5s5pgtaA1Hk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Noise Removal**"
      ],
      "metadata": {
        "id": "ciu98-NWBNhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Check minimum value before cleaning\n",
        "min_before = data['targeted_productivity'].min()\n",
        "print(f\"Lowest targeted_productivity before cleaning: {min_before}\")\n",
        "\n",
        "# 2. Remove unrealistic or noisy values (below 0.1)\n",
        "data_cleaned = data[data['targeted_productivity'] >= 0.1].reset_index(drop=True)\n",
        "\n",
        "# 3. Check minimum value after cleaning\n",
        "min_after = data_cleaned['targeted_productivity'].min()\n",
        "print(f\"Lowest targeted_productivity after cleaning: {min_after}\")\n",
        "\n",
        "# 4. Summary\n",
        "removed_count = len(data) - len(data_cleaned)\n",
        "print(f\"\\nNoise removal complete. {removed_count} data point removed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOkoRAGnJ4MD",
        "outputId": "4475cafe-b6d3-4e30-9e3f-bcaccbf0a3b8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowest targeted_productivity before cleaning: 0.07\n",
            "Lowest targeted_productivity after cleaning: 0.35\n",
            "\n",
            "Noise removal complete. 1 data point removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Check maximum value before cleaning\n",
        "max_before = data['over_time'].max()\n",
        "print(f\"Highest over_time before cleaning: {max_before}\")\n",
        "\n",
        "# 2. Remove values greater than 25,000 (noise)\n",
        "data_cleaned = data[data['over_time'] <= 25000].reset_index(drop=True)\n",
        "\n",
        "# 3. Check maximum value after cleaning\n",
        "max_after = data_cleaned['over_time'].max()\n",
        "print(f\"Highest over_time after cleaning: {max_after}\")\n",
        "\n",
        "# 4. Summary\n",
        "removed_count = len(data) - len(data_cleaned)\n",
        "print(f\"\\nNoise removal complete. {removed_count} data points removed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIzeLG02XrMG",
        "outputId": "aafa0242-04b1-467c-9c79-4b773c5cdfb5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highest over_time before cleaning: 25920\n",
            "Highest over_time after cleaning: 15120\n",
            "\n",
            "Noise removal complete. 1 data points removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MnbnY293YzKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of the Technique (why and how it was applied and which attributes were selected for it):**\n",
        "\n",
        "Noise removal was applied to eliminate unrealistic values that could distort the analysis and lead to incorrect conclusions. This technique was chosen after exploratory data analysis (EDA) revealed anomalies in over_time and targeted_productivity attributes. For instance, one record showed an over_time value of 25,000 minutes, which is unrealistic even when distributed across all 54 team members, and certain entries in targeted_productivity were below 0.1, which is implausibly low and likely due to data entry errors. To maintain accuracy, these records were removed by keeping only entries where over_time ≤ 25000 and targeted_productivity ≥ 0.1.\n",
        "\n",
        "**Description of Preprocessing Results (and how this technique improved the dataset):**\n",
        "\n",
        "After the removal of unrealistic records, both attributes now reflect feasible and consistent ranges. The lowest targeted_productivity value became 0.35, which is a realistic and achievable productivity level in factory conditions. Similarly, the lowest over_time value now stands at 15,000 minutes, equivalent to about 250 hours, which is a plausible cumulative overtime duration for a full team of 30 employees. This process reduced data distortion, enhanced reliability, and ensured that the dataset now represents realistic target production behavior, leading to more accurate insights and reliable modeling outcomes."
      ],
      "metadata": {
        "id": "11rWM2_MY1Jc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Descretization**"
      ],
      "metadata": {
        "id": "Q73b0Yb5ZV14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define number of bins and labels\n",
        "num_bins = 3\n",
        "bin_labels = ['Low', 'Medium', 'High']\n",
        "\n",
        "# Apply discretization\n",
        "data_cleaned['discretized_actual_productivity'] = pd.cut(\n",
        "    data_cleaned['actual_productivity'],\n",
        "    bins=num_bins,\n",
        "    labels=bin_labels,\n",
        "    include_lowest=True\n",
        ")\n",
        "\n",
        "# Print summary\n",
        "print('-------------------------------------------------------')\n",
        "print('Discretization complete: actual_productivity → discretized_actual_productivity')\n",
        "print('-------------------------------------------------------')\n",
        "print('First few values:')\n",
        "print(data_cleaned[['actual_productivity', 'discretized_actual_productivity']].head())\n",
        "print('-------------------------------------------------------')\n",
        "print('Number of instances for each label:')\n",
        "print('-------------------------------------------------------')\n",
        "print('Class  -- Count ---------------------------------------')\n",
        "print(data_cleaned['discretized_actual_productivity'].value_counts())\n",
        "print('-------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wADvhJEZ05H",
        "outputId": "24828c0c-6da9-4b9b-9787-fb620499606f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------\n",
            "Discretization complete: actual_productivity → discretized_actual_productivity\n",
            "-------------------------------------------------------\n",
            "First few values:\n",
            "   actual_productivity discretized_actual_productivity\n",
            "0             0.940725                            High\n",
            "1             0.886500                            High\n",
            "2             0.800570                          Medium\n",
            "3             0.800570                          Medium\n",
            "4             0.800382                          Medium\n",
            "-------------------------------------------------------\n",
            "Number of instances for each label:\n",
            "-------------------------------------------------------\n",
            "Class  -- Count ---------------------------------------\n",
            "discretized_actual_productivity\n",
            "Medium    691\n",
            "High      344\n",
            "Low       161\n",
            "Name: count, dtype: int64\n",
            "-------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation for the Technique (why and how it was applied and which attributes were selected):**\n",
        "\n",
        "Discretization was applied to convert the continuous actual_productivity feature into categorical labels to better suit classification algorithms like Decision Trees. These algorithms tend to perform better when target variables are discrete, as it allows for clear and interpretable rule-based outputs. The process divided actual_productivity into three categories: Low, Medium, and High, using the pd.cut() function with equal-width binning. This ensured that the full range of productivity values was captured while maintaining simplicity and interpretability, which are important for better understanding the model’s outcomes.\n",
        "\n",
        "**Description of Preprocessing Results (and how this technique improved the dataset):**\n",
        "\n",
        "After discretization, the new column discretized_actual_productivity grouped the data into three balanced categories: Medium (691 instances), High (344 instances), and Low (160 instances). This transformation made the target variable more suitable for Decision Tree classification, as the model could now work with clearly defined classes instead of continuous values. Additionally, the categorization enhanced interpretability by allowing a direct comparison across productivity levels and minimized the impact of small fluctuations in continuous measurements. This made the dataset more reliable and easier to model."
      ],
      "metadata": {
        "id": "RBDLrfGaa1Bj"
      }
    }
  ]
}